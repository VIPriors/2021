---
layout: single
title: Challenges
classes: semiwide
permalink: /challenges/
toc: true
---

<img src='/assets/images/logo_challenge.png' style='display: block; margin: 0 auto; width: 40%; min-width: 200px;' />

_The challenges have concluded. We thank all participants and welcome all to participate in next year's challenges!_

We present the **"Visual Inductive Priors for Data-Efficient Computer Vision"** challenges. We offer four challenges, where models are to be trained from scratch, and we reduce the number of training samples to a fraction of the full set. The winners of each challenge are invited to present their winning method at the VIPriors workshop presentation at ECCV 2020. The four data-deficient challenges are:

1. [Image classification](https://competitions.codalab.org/competitions/23713) on [ImageNet](http://www.image-net.org/)
2. [Semantic segmentation](https://competitions.codalab.org/competitions/23712) on [Cityscapes](https://www.cityscapes-dataset.com/)
3. [Object detection](https://competitions.codalab.org/competitions/23661) on [MS COCO](http://cocodataset.org/#home)
4. [Action recognition](https://competitions.codalab.org/competitions/23706) on [UFC-101](https://www.crcv.ucf.edu/research/data-sets/ucf101/)

These tasks were chosen to encourage researchers of arbitrary background to participate: no giant GPU clusters are needed, nor will training for a long time yield much improvement over the baseline results.

## Final rankings

Listed below are the final rankings for each challenge, after reviewing all submissions for which a technical report or paper was received. We congratulate the winners! For more details on the final rankings, please visit the correspondig CodaLab competition for each challenge.

### Image Classification

1. <span class='list-marking'>Pengfei Sun, Xuan Jin, Wei Su, Yuan He, Hui Xue, Quan Lu. *Alibaba Group*</span>

   - Paper: [A visual inductive priors framework for data-efficient image classification.](https://openreview.net/forum?id=Vh-OGLzvNeo)
2. (Shared) Zhipeng Luo, Ge Li, Zhiguang Zhang. *DeepBlue Technology (Shanghai) Co., Ltd*
   - Paper: [A Technical Report for VIPriors Image Classification Challenge](https://arxiv.org/abs/2007.08722)

   (Shared) Zhao Bingchen, Wen Xin. *Megvii Research Nanjing, Tongji University*
   - Paper: [Distilling Visual Priors from Self-Supervised Learning](https://openreview.net/forum?id=8V9lE-zP0ZL)
3. Byeongjo Kim, Chanran Kim, Jaehoon Lee, Jein Song, Gyoungsoo Park. *Zuminternet*
   - Report: [Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble](https://arxiv.org/abs/2007.07805)
4. *Samsung-SLSI-MSL-SS* (aka CodaLab user Ben1365)
   - Report: [Diversification is All You Need: Towards Data Efficient Image Understanding](https://openreview.net/forum?id=UPbbSsBzfEW)

[Full results on CodaLab](https://competitions.codalab.org/competitions/23713)

### Semantic Segmentation

1. <span class='list-marking'>Chen Weitao, Wang Zhibing. *Alibaba Group*</span>
   - Report: [Multi-level tail pixel cutmix and scale attention for long-tailed scene parsing](https://openreview.net/forum?id=GHaQlkoNM-p)
2. Qingfeng Liu, Behnam Babagholami Mohamadabadi, Mostafa El-Khamy, Jungwon Lee. *Samsung-SLSI-MSL-SS*
   - Report: [Diversification is All You Need: Towards Data Efficient Image Understanding](https://openreview.net/forum?id=UPbbSsBzfEW)
3. Chih-Chung Hsu, Hsin-Ti Ma. *National Pingtung University of Science and Technology*
   - Report: [Edge-Preserving Guided Semantic Segmentation for VIPriors Challenge](/assets/downloads/Edge_Preserving_Guided_Semantic_Segmentation.pdf)
4. V. Buğra Yeşilkaynak, Yusuf H. Sahin; G. Unal. *Istanbul Technical University, Computer Engineering*
   - Report: [EfficientSeg: An Efficient Semantic Segmentation Network](https://openreview.net/forum?id=s-OSwnzXvEi)
5. Rafal Pytel, Tomasz Motyka. *Delft University of Technology*
   - Report: [Data-efficient semantic segmentation via extremely perturbed data augmentation](/assets/downloads/Data-efficient_semantic_segmentation_via_extremely_perturbed_data_augmentation.pdf)

[Full results on CodaLab](https://competitions.codalab.org/competitions/23712)

### Object Detection

1. <span class='list-marking'>Fei Shen, Xin He, Mengwan Wei, Yi Xie. *Huaqiao University, Wuhan University Of Technology*</span>
   - Report: [A Competitive Method to VIPriors Object Detection Challenge](/assets/downloads/a_competitive_method_to_vipriors_object_detection_challenge.pdf)
2. Yinzheng Gu, Yihan Pan, Shizhe Chen. *Jilian Technology Group*
   - Report: [2nd Place Solution to ECCV 2020 VIPriors Object Detection Challenge](https://arxiv.org/abs/2007.08849)
3. Zhipeng Luo, Lixuan Che. *DeepBlue Technology (Shanghai) Co., Ltd*
   - Report: [VIPriors Object Detection Challenge](https://arxiv.org/abs/2007.08170)

[Full results on CodaLab](https://competitions.codalab.org/competitions/23661)

### Action Recognition

1. <span class='list-marking'>Ishan Dave, Kali Carter, Mubarak Shah. *Center for Research in Computer Vision (CRCV), University of Central Florida, LeTourneau University*</span>
   - Report: ["Kallis" CRCV VIPriors challenge submission](/assets/downloads/report_kallis.pdf)
2. Haoyu Chen, Zitong Yu, Xin Liu, Wei Peng, Yoon Lee, Guoying Zhao. *University of Oulu, Delft University of Technology*
   - Report: [2nd Place Scheme on Action Recognition Track of ECCV 2020 VIPriors Challenges: An Efficient Optical Flow Stream Guided Framework](https://openreview.net/forum?id=R6YWiPVOQBo)
3. Zhipeng Luo, Dawei Xu, Zhiguang Zhang. *DeepBlue Technology (Shanghai) Co., Ltd*
   - Report: [Challenge report: VIPriors Action Recognition Challenge](https://arxiv.org/abs/2007.08180)
4. Taeoh Kim, Hyeongmin Lee, MyeongAh Cho, Ho Seong Lee, Dong Heon Cho, Sangyoun Lee. *Yonsei University, Cognex Deep Learning Lab*
   - Paper: [Learning Temporally Invariant and Localizable Features via Data Augmentation for Video Recognition](https://openreview.net/forum?id=tqz0rQvz_58)

[Full results on CodaLab](https://competitions.codalab.org/competitions/23706)

## Important dates

- ~~Challenges open: March 11, 2020~~
- ~~Challenges close: July 10, 2020~~
- ~~Technical reports due: July 17, 2020~~
- ~~Winners announced: July 24, 2020~~

*The challenge has been completed! Please see the [final rankings](#final-rankings) above.*

## Data

As training data for these challenges we use subsets of publicly available datasets. We do not directly provide the data but instead expose tooling to generate the subsets from the canonical versions of the publicly available full datasets through our toolkit. Please refer to *Resources* for details.

## Rules

- We prohibit the use of other data than the provided training data, i.e., **no pre-training**, no transfer learning.
- For submissions on CodaLab to qualify to the challenge we require the authors submit either a **technical report** or a full paper about their final submission. See details below under "Report". Submissions without a report or paper associated do not qualify to the competition.
- Top contenders in the challenge may be required to submit their submissions to peer review to ensure reproducability and that the rules of the challenge were followed. The organizers will contact contenders for this when necessary after the challenges close.
- Organizers retain the right to disqualify any submissions that violate these rules.
- The winners of each of the four challenges will get an **opportunity to present** their method at the VIPriors workshop at ECCV 2020. The organizers will contact contenders that are eligible for this opportunity after the challenges close.

<!--
## Report

For the submission on CodaLab to qualify for the competition, we require the authors to submit a technical report of at least three pages about the submission. The deadline for these reports is July 17th, the same date as the workshop paper deadline. Authors are to submit their report to ArXiv and submit the link to us using the form linked below. Those unable to submit to Arxiv can email their report to [the emailaddress listed under "Organizers"](/#organizers). Please use the same format as for the paper track. After the conference we will publish the links to the technical reports on the workshop website.

Authors that are already submitting a paper about the submission to the workshop paper track are not required to submit a technical report. Instead, they are to use the same submission form to refer the challenge organizers to their submitted paper.

Link to submission form: [Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdzTQXw60TL70hxDW1WsEp52AUI2CVe3QX7T0Qi420Gey9H9Q/viewform?usp=sf_link).


## Submission

Each of the four challenges are hosted on CodaLab, a public platform for AI challenges. Submissions must be made by uploading files containing predictions according to the format defined in the toolkit (see *Resources* for details) to the challenge pages listed below.

Please find the challenges here:

- [Image classification](https://competitions.codalab.org/competitions/23713)
- [Semantic segmentation](https://competitions.codalab.org/competitions/23712)
- [Object detection](https://competitions.codalab.org/competitions/23661)
- [Action recognition](https://competitions.codalab.org/competitions/23706)
- -->

## Resources

To accommodate submissions to the challenges we provide a toolkit that contains

- Python tools for generating the appropriate training and validation data;
- documentation of the required submission format for the challenges;
- implementations of the baseline models for each challenge.

See [the GitHub repository of the toolkit here](https://github.com/VIPriors/vipriors-challenges-toolkit).

## Questions

If you have any questions, please first check the Frequently Asked Questions in [the toolkit repository](https://github.com/VIPriors/vipriors-challenges-toolkit). If your question persists, you can ask it on the forums of the specific challenge on the CodaLab website. If you need to ask us a question in private, you can email us at vipriors-ewi AT tudelft DOT nl.