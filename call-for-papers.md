---
layout: single
title: Call for papers
permalink: /call-for-papers/
---

Data is fueling deep learning. Data is costly to gather and expensive to annotate. Training on massive datasets has a huge energy consumption adding to our carbon footprint. This workshop aims beyond the few very large companies that can accommodate ML on this scale to the long tail of smaller companies and universities with smaller datasets and smaller hardware clusters. We focus on data efficiency through visual inductive priors.

Excellent recent research investigates data efficiency in deep networks by exploiting other data sources such as unsupervised learning, re-using existing datasets, or synthesizing artificial training data. Not much attention is given on how to overcome the data dependency by adding prior knowledge to deep nets. As a consequence, all knowledge has to be (re-)learned implicitly from data, making deep networks hard to understand black boxes. This workshop aims to remedy this gap by investigating how to flexibly pre-wire deep networks with generic visual innate knowledge structures, which allows to incorporate hard won existing knowledge from physics such as light reflection or geometry.

We encourage submissions covering but not limited any data-efficient method in the following topics:

- Object classification
- Object detection
- Segmentation
- Action recognition
- ...

*TODO*

**Important dates**

- Submission deadline: *TBD*
- Notification of acceptance: *TBD*
- Camera-ready deadline: *TBD*
- Workshop: August 23/28 2020 (*TBD*)

**Submission guidelines**

- Submissions must be entered in *TBD*
- Page limit & format: *TBD*
- Review format: *TBD*
- Paper selection criteria: *TBD*
- Paper publication: *TBD*