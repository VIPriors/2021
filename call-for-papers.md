---
layout: single
title: Call for papers
permalink: /call-for-papers/
---

Data is fueling deep learning. Data is costly to gather and expensive to annotate. Training on massive datasets has a huge energy consumption adding to our carbon footprint. This workshop aims beyond the few very large companies that can accommodate ML on this scale to the long tail of smaller companies and universities with smaller datasets and smaller hardware clusters. We focus on data efficiency through visual inductive priors.

Excellent recent research investigates data efficiency in deep networks by exploiting other data sources such as unsupervised learning, re-using existing datasets, or synthesizing artificial training data. Not much attention is given on how to overcome the data dependency by adding prior knowledge to deep nets. As a consequence, all knowledge has to be (re-)learned implicitly from data, making deep networks hard to understand black boxes. This workshop aims to remedy this gap by investigating how to flexibly pre-wire deep networks with generic visual innate knowledge structures, which allows to incorporate hard won existing knowledge from physics such as light reflection or geometry.

We encourage submissions covering but not limited to the following topics:

- Improving data efficiency of Deep Computer Vision methods using prior knowledge about the task domain
- Analysis on the properties of Deep Learning representations as they relate to visual inductive priors
- Transformation-equivariant image representations, e.g. scale-equivariance, rotation-equivariance, etc.
- Color invariants/constants in Deep Learning
- Object persistence between video frames
- Shape-based representations for Deep Learning
- Texture/shape bias in Convolutional Neural Networks
- Alternative compact filter bases for Deep Learning
- Capsule Networks
- *TODO*

**Important dates**

- Submission deadline: *TBD*
- Notification of acceptance: *TBD*
- Camera-ready deadline: *TBD*
- Workshop: August 23/28 2020 (*TBD*)

**Submission guidelines**

- Submissions must be entered in *TBD*
- Page limit & format: *TBD*
- Review format: *TBD*
- Paper selection criteria: *TBD*
- Paper publication: *TBD*